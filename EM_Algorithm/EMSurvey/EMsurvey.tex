\documentclass[11pt]{article}
%Gummi|061|=)
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\title{\textbf{EM Algorithm in SR}}
\author{Tianxing He}
\date{}
\begin{document}
\bibliographystyle{plain}
\maketitle

\section{Introduction}
Since its formal introduction in 1977 by Dempster et al. \cite{ADempsterEM}, the EM algorithm has become a standard methodology for ML estimation. The EM Algorithm is steadily becoming more and more popular and is being used in an increasing number of applications. The first publications in IEEE journals making reference to the EM algorithm appeared in 1988 and dealt with the problem of tomographic reconstruction of photon limited images \cite{1977Posit}, \cite{1988Bayes}. Since then, the EM algorithm has become a popular tool for statistical signal processing used in a wide range of applications, such as recovery and segmentation of images and video, image modelling, carrier frequency synchronization, and channel estimation in communications and speech recognition. \\
Assume that x are the observations and $\theta$ the unknown parameters of a model that generated x. The ML estimate is obtained as 
\begin{equation}
\hat{\theta}_{ML}= \argmax_{\theta}L(\theta) = ln(p(x;\theta))
\end{equation}
However, in many cases the likelihood function $p(x;\theta)$ is complex and is either difficult or immpossible to directly optimize. In such cases the computation of this likelihood is greatly facilitated by the introduction of hidden variables z. These random variables act as links that connect the observations to the unknown parameters via Bayes’ law. The choice of hidden variables is problem dependent. However, as their name suggests, these variables are not observed and they provide enough information about the observations so that the conditional probability $p(x|z)$ is easy to compute. Apart from this role, hidden variables play
another role in statistical modeling. They are an important part of the probabilistic mechanism that is assumed to have generated the observations and can be described very succinctly by a graph that is termed “graphical model.” 
Once hidden variables and a prior probability for them $p(z;\theta)$ have been introduced, one can obtain the likelihood or the marginal likelihood as it is called at times by integrating
out (marginalizing) the hidden variables according to
\begin{equation}{\label{2}}
L(\theta) = lnp(x;\theta) = ln \int p(x,z;\theta) = ln \int P(x|z;\theta)p(z;\theta) 
\end{equation}
Despite the simplicity of the above formulation, in most cases of interest the integral in (\ref{2}) is either impossible or very difficult to compute in closed form. Thus, the main effort in Bayesian Inference is concentrated on techniques that allow us to bypass or approximately evaluate this integral.  \\
Such methods can be classified into two broad categories\cite{LifeAfter}. The first is numerical sampling methods also known as Monte Carlo techniques and the second is deterministic approximations. Here we won't address Monte Carlo methods\cite{2003MC}, \cite{MCMC}. On the other hand, the EM algorithm is a Bayesian inference methodology that assumes knowledge of the posterior $p(z|x;\theta)$ and iteratively maximizes the likelihood function without explicitly computing it.\\
One of the most insightful explanations of EM, that provides a deeper understanding of its operation than the intuition of alternating between variables, is in terms of lower-bound maximization \cite{neal}.\\
Assuming now we somehow have a parameter $\theta^{(p)}$, using JenSen's inequaility and following equation(\ref{2}), we have
\begin{equation}{\label{3}}
L(\theta) = ln \int P(x, z;\theta) = ln \int \frac{P(x, z;\theta)q(z)}{q(z)}
\geq \int q(z) ln\frac{P(x,z;\theta)}{q(z)}
\end{equation}
Where $q(z)$ could be any distribution. For EM Algorithm, we take $q(z) = p(z|x; \theta^{(p)})$, so
\begin{equation}
\begin{split}
\int q(z) ln\frac{P(x,z;\theta)}{q(z)} = \int p(z|x; \theta^{(p)}) ln\frac{P(x,z;\theta)}{p(z|x; \theta^{(p)})} \\ 
= \int p(z|x; \theta^{(p)}) lnP(x,z;\theta) - \int p(z|x; \theta^{(p)})lnp(z|x; \theta^{(p)})
\end{split}
\end{equation}
Notice the second term is constant. We now define the \textbf{auxiliary function}
\begin{equation}
Q(\theta, \theta^{(p)}) = \int p(z|x; \theta^{(p)}) lnP(x,z;\theta)
\end{equation}
Assume we have $\theta^{(p)}$, in the E-step, we deride the anxiliary function, then in the M-step, we maximize the anxiliary function to get $\theta^{(p+1)}$. One get the inituition of EM in equation (\ref{3}) : we are maximizing the lower bound of $L(\theta)$.
\begin{equation}
\begin{split}
&EM~Alorithm \\
E-Step &: compute~p(z|x;\theta^{old})\\
M-Step &: Evaluate~\theta^{new}=\argmax_{\theta}Q(\theta,\theta^{old})
\end{split}
\end{equation}


\section{Properities of the EM algorithm}
\subsection{Monotonous Increase of the Likelihood}
We follow the exposition of the EM in \cite{PRML} and \cite{1998RM}. It is straightforward to show that the log-likelihood can be written as
\begin{equation}{\label{4}}
lnp(x;\theta)=F(q,\theta) + KL(q||p)
\end{equation}
where
\begin{equation}
F(q,\theta)=\int q(z)ln\frac{p(x,z;\theta)}{q(z)}
\end{equation}
and
\begin{equation}
KL(q||p)=\int -q(z)ln\frac{p(z|x;\theta)}{q(z)}
\end{equation}
Notice that we always have $KL(q||p) \geq 0$ as long as $q$ is a distribution.\\
Now, assuming we have a $\theta^{(p)}$, and let $q(z) = p(z|x;\theta^{(p)})$, we can get $\theta^{(p+1)}$ through an iteration:
\begin{equation}
\theta^{(p+1)} = \argmax_{\theta}F(q, \theta)
\end{equation}
Notice this is equal to maximizing the anxiliary function.\\
Now, one can easily find out that if we just take $\theta^{(p+1)}=\theta^{(p)}$, we have$F(q, \theta^{(p+1)}) = L(\theta^{(p)})$, because $KL(q||p)=0$. Since we are maximizing $F$, we have
\begin{equation}
F(q, \theta^{(p+1)}) \geq L(\theta^{(p)})
\end{equation}
Remembering that we always have $KL(q||p) \geq 0$, we get
\begin{equation}
L(\theta^{(p+1)}) \geq L(\theta^{(p)})
\end{equation}
\subsection{Convergence to a Local Maxima}
With the exception of a few specific cases, the EM algorithm is not guaranteed to converge to a global maximizer of the likehood\cite{EMTour}. Under some regularity conditions on the likelihood $L(\theta)$ and on the parameters set $\Theta$, it is possible, however, to show the parameter sequence obtained by EM converges to a local maximizer of $L(\theta)$ or at least, to a stationary point of $L(\theta)$. Necessary conditions for the convergence of the EM algorithm and related theorems can be found in \cite{EMCov}.
\section{Variants of the EM Algorithm}
\subsection{Penalized Likelihood Estimation}
The EM algorithm can be straightforwardly modified to compute penalized likelihood estimates, that is, estimates of the form
\begin{equation}
\theta = \argmax_{\theta}[L(\theta)+G(\theta)]
\end{equation}
The penalty term $G(\theta)$ could represent, for example, the logarithm of a prior on if a Bayesian approach is used and the maximum a posteriori(MAP) estimate of is desired instead of the ML estimate.(Someone put it "poor man’s bayesian inference\cite{LifeAfter}") The EM algorithm for penalized-likelihood estimation can be obtained by replacing the M-step with
\begin{equation}
\theta^{(p+1)}=\argmax_{\theta}[Q(\theta, \theta^{(p)}) +G(\theta)]
\end{equation}
It is straightforward to see that the monotonicity property is preserved. Examples of penalized likelihood estimation to be found in \cite{penalized}.
\subsection{Variational EM}
One can bypass the requirement of exactly knowing $p(z|x;\theta)$ by assuming an appropriate q(z) in the decomposition (\ref{4})\cite{LifeAfter}. In the E-step q(z) is found such that it maximizes $F(q,\theta)$ keeping $\theta$ fixed. To perform this maximization, a particular
form of q(z) must be assumed. In certain cases it is possible to assume knowledge of the form of $q(z;\omega)$, where ω is a set of ω parameters. Thus, the lower bound $F(\omega,\theta)$ becomes a function of these parameters and is maximized with respect to ω in the E-step and with respect to θ in the M-step. The process can be summarized as below\cite{gentle}:
\begin{equation}
\begin{split}
&Variational~EM \\
Variational~E-Step &: compute~\hat{q} = \argmax_{q}F(q, \theta^{old})\\
Variational~M-Step &: Evaluate~\theta^{new}=\argmax_{\theta}F(\hat{q},\theta)
\end{split}
\end{equation}
One can easily deduce the monotonicity property is still preserved, since we can first prove
\begin{equation}
F(\hat{q},\theta^{old}) \geq L(\theta^{old})
\end{equation}
and then
\begin{equation}
F(\hat{q}, \theta^{new}) \geq F(\hat{q}, \theta^{old})
\end{equation}
because of the {argmax}.\\
Although there are no approximations in the variational theory, variational methods can be used to find approximate solutions in Bayesian inference problems. This is done by assuming that the functions over which optimization is performed have specific forms. For example, we can assume only quadratic functions or functions that are linear combinations of fixed basis functions. For Bayesian inference, a particular form that has been used with great success is the factorized one, see \cite{1var} and \cite{2var}. In which case we assume
\begin{equation}
q(z)=\prod^{M}_{i=1}q_i(z_i)
\end{equation} 
From that assumption we could deride \cite{LifeAfter}
\begin{equation}
q_j^*(z_j)=\frac{exp(<lnp(x,z;\theta)>_{i\neq j})}{\int exp(<lnp(x,z;\theta)>_{i\neq j})dz_j}
\end{equation}





\bibliography{ref}

\end{document}
